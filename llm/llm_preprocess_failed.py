"""
ì‹¤íŒ¨ í•­ëª© LLM ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

outcome_normalized_failed í…Œì´ë¸”ì—ì„œ ì‹¤íŒ¨ í•­ëª©ì„ ì¶”ì¶œí•˜ì—¬
Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ measureì™€ timeframe íŒŒì‹±ì„ ì‹œë„í•©ë‹ˆë‹¤.
"""

import os
import json
import time
from typing import Dict, Optional, List
import psycopg2
from psycopg2.extras import RealDictCursor, execute_batch
from dotenv import load_dotenv
from llm_config import (
    get_api_keys, GEMINI_MODEL, PREPROCESS_RULES,
    MAX_REQUESTS_PER_MINUTE, BATCH_SIZE, MAX_RETRIES, RETRY_DELAY
)
from llm_prompts import get_preprocess_failed_prompt

load_dotenv()

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432'),
    'database': os.getenv('DB_NAME', 'clinicaltrials'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', '')
}


def get_db_connection():
    """PostgreSQL ì—°ê²° ìƒì„±"""
    return psycopg2.connect(**DB_CONFIG)


def call_gemini_api(prompt: str) -> Optional[Dict]:
    """Gemini API í˜¸ì¶œ (ì—¬ëŸ¬ API í‚¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„, 429 ì—ëŸ¬ ì‹œ ìë™ ì „í™˜)"""
    api_keys = get_api_keys()
    if not api_keys:
        print("[ERROR] GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        return None
    
    # í˜„ì¬ ì „ì—­ í‚¤ ì¸ë±ìŠ¤ë¶€í„° ì‹œì‘
    import llm_config
    start_key_index = llm_config._current_key_index
    
    last_error = None
    
    # ëª¨ë“  í‚¤ë¥¼ ì‹œë„ (6ê°œ í‚¤ë©´ 0,1,2,3,4,5 ì´ 6ë²ˆ)
    for attempt in range(len(api_keys)):
        key_index = (start_key_index + attempt) % len(api_keys)
        
        try:
            # íŠ¹ì • í‚¤ë¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            from google import genai
            client = genai.Client(api_key=api_keys[key_index])
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt
            )
            
            # ì„±ê³µ ì‹œ ì „ì—­ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (í‚¤ ë³€ê²½ ì‹œ ê·œì¹™ ë‹¤ì‹œ ë³´ë‚´ê¸° ìœ„í•´)
            if llm_config._current_key_index != key_index:
                llm_config._previous_key_index = llm_config._current_key_index
                llm_config._current_key_index = key_index
            else:
                llm_config._current_key_index = key_index
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content = response.text.strip()
            
            # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
            if content.startswith('```'):
                # ì½”ë“œ ë¸”ë¡ ì œê±°
                lines = content.split('\n')
                content = '\n'.join(lines[1:-1]) if len(lines) > 2 else content
            
            try:
                parsed = json.loads(content)
                return parsed
            except json.JSONDecodeError as e:
                print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨ (í‚¤ {key_index + 1}/{len(api_keys)}): {e}")
                print(f"  ì‘ë‹µ ë‚´ìš©: {content[:200]}")
                return None
            
        except Exception as e:
            error_str = str(e)
            last_error = e
            
            # 429 ì—ëŸ¬ (RESOURCE_EXHAUSTED) ì²´í¬
            if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str.upper():
                print(f"âš ï¸  API í‚¤ {key_index + 1}/{len(api_keys)}ì—ì„œ 429 ì—ëŸ¬ ë°œìƒ (ì‹œë„ {attempt + 1}/{len(api_keys)}): {error_str}")
                
                # ë§ˆì§€ë§‰ ì‹œë„(6ê°œ í‚¤ë©´ attempt==5)ì¸ ê²½ìš° ì¢…ë£Œ
                if attempt == len(api_keys) - 1:
                    print(f"[ERROR] ëª¨ë“  API í‚¤({len(api_keys)}ê°œ)ê°€ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    llm_config._previous_key_index = llm_config._current_key_index
                    llm_config._current_key_index = key_index
                    llm_config._all_keys_exhausted = True
                    return None
                else:
                    next_key_index = (start_key_index + attempt + 1) % len(api_keys)
                    print(f"ğŸ”„ ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜í•©ë‹ˆë‹¤ (í‚¤ {next_key_index + 1}/{len(api_keys)})")
                    continue  # ë‹¤ìŒ í‚¤ë¡œ ì¬ì‹œë„
            else:
                # 429ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì—ëŸ¬ëŠ” í˜„ì¬ í‚¤ì—ì„œ ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë°˜í™˜
                print(f"[ERROR] Gemini API ì˜¤ë¥˜ (í‚¤ {key_index + 1}/{len(api_keys)}): {error_str}")
                return None
    
    # ëª¨ë“  í‚¤ ì‹œë„ ì‹¤íŒ¨ (ì´ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šì•„ì•¼ í•¨)
    print(f"[ERROR] ëª¨ë“  API í‚¤({len(api_keys)}ê°œ) ì‹œë„ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì—ëŸ¬: {str(last_error)}")
    llm_config._all_keys_exhausted = True
    return None


def preprocess_batch_outcomes(outcomes: List[Dict]) -> List[Dict]:
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ failed outcomeë“¤ì„ LLMìœ¼ë¡œ ì „ì²˜ë¦¬ (ê·œì¹™ ìºì‹± ì ìš©)"""
    if not outcomes:
        return []
    
    import llm_config
    
    # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìµœì†Œí™”)
    items = []
    for outcome in outcomes:
        oid = outcome['outcome_id']
        mr = outcome.get('measure_raw', '')
        dr = outcome.get('description_raw', '')
        tr = outcome.get('time_frame_raw', '')
        # ë¹ˆ ê°’ ìƒëµí•˜ì—¬ ë” ì§§ê²Œ
        parts = [f"{oid}"]
        if mr: parts.append(f"M:{mr}")
        if dr: parts.append(f"D:{dr}")
        if tr: parts.append(f"T:{tr}")
        item_str = "|".join(parts)
        items.append(item_str)
    
    # API í‚¤ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ê·œì¹™ ìºì‹±)
    current_key_idx = llm_config._current_key_index
    previous_key_idx = llm_config._previous_key_index
    include_rules = (current_key_idx != previous_key_idx or previous_key_idx == -1)
    
    # í‚¤ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ í˜¸ì¶œì„ ìœ„í•´)
    if include_rules:
        llm_config._previous_key_index = current_key_idx
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    items_text = '\n'.join(items)
    prompt = get_preprocess_failed_prompt(items_text)
    
    result = call_gemini_api(prompt)
    
    if not result:
        # API ì‹¤íŒ¨ ì‹œ ëª¨ë‘ null ì²˜ë¦¬
        return [{
            'outcome_id': outcome['outcome_id'],
            'llm_parsed_measure_code': None,
            'llm_parsed_time_value': None,
            'llm_parsed_time_unit': None,
            'llm_validation_confidence': None,
            'llm_validation_notes': 'API í˜¸ì¶œ ì‹¤íŒ¨'
        } for outcome in outcomes]
    
    # ê²°ê³¼ íŒŒì‹± (ë°°ì—´ë¡œ ì‘ë‹µ ë°›ìŒ)
    results = []
    if isinstance(result, list):
        # outcome_idë¡œ ë§¤í•‘
        result_map = {r.get('outcome_id'): r for r in result if 'outcome_id' in r}
        for outcome in outcomes:
            outcome_id = outcome['outcome_id']
            if outcome_id in result_map:
                r = result_map[outcome_id]
                results.append({
                    'outcome_id': outcome_id,
                    'llm_parsed_measure_code': r.get('measure_code'),
                    'llm_parsed_time_value': r.get('time_value'),
                    'llm_parsed_time_unit': r.get('time_unit'),
                    'llm_validation_confidence': r.get('confidence'),
                    'llm_validation_notes': r.get('notes')
                })
            else:
                results.append({
                    'outcome_id': outcome_id,
                    'llm_parsed_measure_code': None,
                    'llm_parsed_time_value': None,
                    'llm_parsed_time_unit': None,
                    'llm_validation_confidence': None,
                    'llm_validation_notes': 'ì‘ë‹µì— outcome_id ì—†ìŒ'
                })
    else:
        # ë‹¨ì¼ ì‘ë‹µì¸ ê²½ìš° (í•˜ìœ„ í˜¸í™˜ì„±)
        if outcomes:
            results.append({
                'outcome_id': outcomes[0]['outcome_id'],
                'llm_parsed_measure_code': result.get('measure_code'),
                'llm_parsed_time_value': result.get('time_value'),
                'llm_parsed_time_unit': result.get('time_unit'),
                'llm_validation_confidence': result.get('confidence'),
                'llm_validation_notes': result.get('notes')
            })
    
    return results


def update_llm_results(conn, results: List[Dict]):
    """LLM ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸"""
    if not results:
        return
    
    update_sql = """
        UPDATE outcome_normalized
        SET 
            llm_parsed_measure_code = %(llm_parsed_measure_code)s,
            llm_parsed_time_value = %(llm_parsed_time_value)s,
            llm_parsed_time_unit = %(llm_parsed_time_unit)s,
            llm_validation_confidence = %(llm_validation_confidence)s,
            llm_validation_notes = %(llm_validation_notes)s,
            parsing_method = CASE 
                WHEN %(llm_parsed_measure_code)s IS NOT NULL 
                     AND %(llm_parsed_time_value)s IS NOT NULL 
                     AND %(llm_parsed_time_unit)s IS NOT NULL 
                THEN 'LLM' 
                ELSE parsing_method 
            END
        WHERE outcome_id = %(outcome_id)s
    """
    
    with conn.cursor() as cur:
        execute_batch(cur, update_sql, results, page_size=100)
        conn.commit()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    print("=" * 80)
    print("[START] ì‹¤íŒ¨ í•­ëª© LLM ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    
    api_keys = get_api_keys()
    if not api_keys:
        print("\n[ERROR] GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("í™˜ê²½ë³€ìˆ˜ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì— ì¶”ê°€í•˜ì„¸ìš”.")
        print("ì—¬ëŸ¬ í‚¤ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ GEMINI_API_KEY_2, GEMINI_API_KEY_3 ë“±ì„ ì¶”ê°€í•˜ì„¸ìš”")
        sys.exit(1)
    
    print(f"\n[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {len(api_keys)}ê°œ")
    print(f"[INFO] ì‚¬ìš© ëª¨ë¸: {GEMINI_MODEL}")
    
    # ì²˜ë¦¬í•  í•­ëª© ìˆ˜ ì œí•œ (ì˜µì…˜)
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else None
    
    try:
        conn = get_db_connection()
        
        # ì‹¤íŒ¨ í•­ëª© ì¡°íšŒ - ì „ì²´ ì¡°íšŒ
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            query = """
                SELECT 
                    outcome_id,
                    measure_raw,
                    description_raw,
                    time_frame_raw
                FROM outcome_normalized_failed
                WHERE llm_parsed_measure_code IS NULL
                  AND llm_parsed_time_value IS NULL
                  AND llm_parsed_time_unit IS NULL
                ORDER BY outcome_id
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cur.execute(query)
            failed_outcomes = cur.fetchall()
        
        total_count = len(failed_outcomes)
        print(f"\n[INFO] ì²˜ë¦¬í•  ì‹¤íŒ¨ í•­ëª©: {total_count:,}ê°œ")
        
        if total_count == 0:
            print("[INFO] ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            conn.close()
            return
        
        # LLM ì „ì²˜ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬)
        print("\n[STEP 1] LLM ì „ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {})...".format(BATCH_SIZE))
        all_results = []
        success_count = 0
        failed_count = 0
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ì—ì„œ ë‚˜ëˆ„ê¸°)
        for batch_start in range(0, total_count, BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, total_count)
            batch_outcomes = failed_outcomes[batch_start:batch_end]
            batch_num = (batch_start // BATCH_SIZE) + 1
            total_batches = (total_count + BATCH_SIZE - 1) // BATCH_SIZE
            
            print(f"  ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘: {batch_start + 1:,}~{batch_end:,}ë²ˆì§¸ í•­ëª©")
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ í™•ì¸
            import llm_config
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í•œë²ˆì— API í˜¸ì¶œ
            batch_results = preprocess_batch_outcomes(batch_outcomes)
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸ (í˜¸ì¶œ ì¤‘ì— ì†Œì§„ë  ìˆ˜ ìˆìŒ)
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            # ê²°ê³¼ ì§‘ê³„
            for result in batch_results:
                all_results.append(result)
                if result['llm_parsed_measure_code'] and result['llm_parsed_time_value']:
                    success_count += 1
                else:
                    failed_count += 1
            
            # Rate limiting (ë°°ì¹˜ë‹¹ 1íšŒ í˜¸ì¶œì´ë¯€ë¡œ ë‹¨ìˆœí™”)
            time.sleep(60 / MAX_REQUESTS_PER_MINUTE)  # ë¶„ë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆìœ¼ë©´ ë°°ì¹˜ ë£¨í”„ë„ ì¤‘ë‹¨
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
        
        results = all_results
        
        print(f"\n[STEP 2] ê²°ê³¼ ì—…ë°ì´íŠ¸ ì¤‘... (ë°°ì¹˜ ë‹¨ìœ„)")
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ DB ì—…ë°ì´íŠ¸
        for batch_start in range(0, len(results), BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, len(results))
            batch_results = results[batch_start:batch_end]
            update_llm_results(conn, batch_results)
        
        print(f"\n[INFO] ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  ì „ì²´: {total_count:,}ê°œ")
        print(f"  ì„±ê³µ (measure_code + time íŒŒì‹±): {success_count:,}ê°œ")
        print(f"  ì‹¤íŒ¨: {failed_count:,}ê°œ")
        
        conn.close()
        
    except Exception as e:
        print(f"\n[ERROR] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        if 'conn' in locals():
            conn.close()


if __name__ == "__main__":
    main()

