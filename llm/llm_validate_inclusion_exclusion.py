"""
LLM ì „ì²˜ë¦¬ ì„±ê³µ í•­ëª© ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ (Inclusion/Exclusion)

inclusion_exclusion_llm_preprocessed í…Œì´ë¸”ì—ì„œ llm_status = 'SUCCESS'ì¸ í•­ëª©ë“¤ì„
LLMìœ¼ë¡œ ê²€ì¦í•˜ì—¬ ë¬¸ì„œí™”í•©ë‹ˆë‹¤.
"""

import os
import json
import time
from datetime import datetime
from typing import Dict, Optional, List
import psycopg2
from psycopg2.extras import RealDictCursor, execute_batch
from dotenv import load_dotenv
from llm_config import (
    get_api_keys, GEMINI_MODEL,
    MAX_REQUESTS_PER_MINUTE, BATCH_SIZE, MAX_RETRIES, RETRY_DELAY
)
from llm_prompts import get_inclusion_exclusion_validation_prompt

load_dotenv()

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432'),
    'database': os.getenv('DB_NAME', 'clinicaltrials'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', '')
}

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def get_db_connection():
    """PostgreSQL ì—°ê²° ìƒì„±"""
    return psycopg2.connect(**DB_CONFIG)


def call_gemini_api(prompt: str) -> Optional[List]:
    """Gemini API í˜¸ì¶œ (ì—¬ëŸ¬ API í‚¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„, 429 ì—ëŸ¬ ì‹œ ìë™ ì „í™˜)"""
    api_keys = get_api_keys()
    if not api_keys:
        print("[ERROR] GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        return None
    
    import llm_config
    start_key_index = llm_config._current_key_index
    last_error = None
    
    for attempt in range(len(api_keys)):
        key_index = (start_key_index + attempt) % len(api_keys)
        
        try:
            from google import genai
            client = genai.Client(api_key=api_keys[key_index])
            # ê²€ì¦ ì‹œ Temperatureë¥¼ 0.0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë³€ë™ì„± ìµœì†Œí™”
            # generate_contentì— temperature íŒŒë¼ë¯¸í„° ì§ì ‘ ì „ë‹¬ ì‹œë„
            try:
                response = client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=prompt,
                    temperature=0.0  # ê²°ì •ë¡ ì  ì¶œë ¥ì„ ìœ„í•´ ìµœì†Œê°’ ì„¤ì •
                )
            except TypeError:
                # temperature íŒŒë¼ë¯¸í„°ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° ê¸°ë³¸ í˜¸ì¶œ
                response = client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=prompt
                )
            
            if llm_config._current_key_index != key_index:
                llm_config._previous_key_index = llm_config._current_key_index
                llm_config._current_key_index = key_index
            else:
                llm_config._current_key_index = key_index
            
            content = response.text.strip()
            
            # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
            if '```' in content:
                import re
                code_block_pattern = r'```(?:json)?\s*\n(.*?)\n```'
                match = re.search(code_block_pattern, content, re.DOTALL)
                if match:
                    content = match.group(1).strip()
                else:
                    content = re.sub(r'```(?:json)?', '', content).strip()
            
            # JSON ë°°ì—´ ì‹œì‘ ë¶€ë¶„ ì°¾ê¸°
            json_start = content.find('[')
            if json_start >= 0:
                content = content[json_start:]
            
            json_end = content.rfind(']')
            if json_end >= 0:
                content = content[:json_end + 1]
            
            content = content.strip()
            
            try:
                parsed = json.loads(content)
                if not isinstance(parsed, list):
                    parsed = [parsed]
                return parsed
            except json.JSONDecodeError as e:
                print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨ (í‚¤ {key_index + 1}/{len(api_keys)}): {e}")
                print(f"  ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì): {content[:500]}")
                
                # ë¶€ë¶„ íŒŒì‹± ì‹œë„: ì™„ì „í•œ JSON ê°ì²´ë“¤ë§Œ ì¶”ì¶œ
                try:
                    import re
                    parsed_items = []
                    
                    # ì™„ì „í•œ JSON ê°ì²´ íŒ¨í„´ ì°¾ê¸° (ì¤‘ì²© êµ¬ì¡° ì§€ì›)
                    # { ... } í˜•íƒœì˜ ì™„ì „í•œ ê°ì²´ë¥¼ ì°¾ìŒ
                    brace_count = 0
                    start_pos = -1
                    current_obj = ""
                    
                    for i, char in enumerate(content):
                        if char == '{':
                            if brace_count == 0:
                                start_pos = i
                            brace_count += 1
                            current_obj += char
                        elif char == '}':
                            current_obj += char
                            brace_count -= 1
                            if brace_count == 0 and start_pos >= 0:
                                # ì™„ì „í•œ ê°ì²´ ë°œê²¬
                                try:
                                    obj = json.loads(current_obj)
                                    if isinstance(obj, dict) and 'nct_id' in obj:
                                        parsed_items.append(obj)
                                except json.JSONDecodeError:
                                    pass
                                current_obj = ""
                                start_pos = -1
                        elif start_pos >= 0:
                            current_obj += char
                    
                    if parsed_items:
                        print(f"  [ë³µêµ¬] {len(parsed_items)}ê°œ í•­ëª©ì„ ë¶€ë¶„ íŒŒì‹±í•˜ì—¬ ë³µêµ¬í–ˆìŠµë‹ˆë‹¤.")
                        return parsed_items
                    else:
                        # ì •ê·œì‹ìœ¼ë¡œë„ ì‹œë„
                        json_objects = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                        for obj_str in json_objects:
                            try:
                                obj = json.loads(obj_str)
                                if isinstance(obj, dict) and 'nct_id' in obj:
                                    parsed_items.append(obj)
                            except json.JSONDecodeError:
                                continue
                        
                        if parsed_items:
                            print(f"  [ë³µêµ¬] {len(parsed_items)}ê°œ í•­ëª©ì„ ì •ê·œì‹ìœ¼ë¡œ ë³µêµ¬í–ˆìŠµë‹ˆë‹¤.")
                            return parsed_items
                        
                except Exception as recover_error:
                    print(f"  [ë³µêµ¬ ì‹¤íŒ¨] {recover_error}")
                
                return None
            
        except Exception as e:
            error_str = str(e)
            last_error = e
            
            if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str.upper():
                print(f"âš ï¸  API í‚¤ {key_index + 1}/{len(api_keys)}ì—ì„œ 429 ì—ëŸ¬ ë°œìƒ (ì‹œë„ {attempt + 1}/{len(api_keys)}): {error_str}")
                
                if attempt == len(api_keys) - 1:
                    print(f"[ERROR] ëª¨ë“  API í‚¤({len(api_keys)}ê°œ)ê°€ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    llm_config._previous_key_index = llm_config._current_key_index
                    llm_config._current_key_index = key_index
                    llm_config._all_keys_exhausted = True
                    return None
                else:
                    next_key_index = (start_key_index + attempt + 1) % len(api_keys)
                    print(f"ğŸ”„ ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜í•©ë‹ˆë‹¤ (í‚¤ {next_key_index + 1}/{len(api_keys)})")
                    continue
            else:
                print(f"[ERROR] Gemini API ì˜¤ë¥˜ (í‚¤ {key_index + 1}/{len(api_keys)}): {error_str}")
                return None
    
    print(f"[ERROR] ëª¨ë“  API í‚¤({len(api_keys)}ê°œ) ì‹œë„ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì—ëŸ¬: {str(last_error)}")
    llm_config._all_keys_exhausted = True
    return None


def format_criteria(criteria) -> str:
    """inclusion/exclusion criteriaë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if not criteria:
        return ''
    try:
        if isinstance(criteria, str):
            criteria = json.loads(criteria)
        if isinstance(criteria, list):
            return json.dumps(criteria, ensure_ascii=False)
    except:
        pass
    return str(criteria)


def validate_batch_single_run(eligibility_list: List[Dict]) -> Dict[str, Dict]:
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ eligibilityCriteriaë¥¼ LLMìœ¼ë¡œ ê²€ì¦ (1íšŒ ì‹¤í–‰)
    
    Args:
        eligibility_list: ê²€ì¦í•  eligibility ë¦¬ìŠ¤íŠ¸
    
    Returns:
        {nct_id: {status, confidence, notes}} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    if not eligibility_list:
        return {}
    
    # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ í™•ì¸
    import llm_config
    if llm_config._all_keys_exhausted:
        # ëª¨ë“  í‚¤ê°€ ì†Œì§„ëœ ê²½ìš° UNCERTAIN ë°˜í™˜
        return {
            eligibility.get('nct_id'): {
                'status': 'UNCERTAIN',
                'confidence': None,
                'notes': '[API_KEYS_EXHAUSTED] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.'
            }
            for eligibility in eligibility_list if eligibility.get('nct_id')
        }
    
    # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
    items = []
    nct_id_map = {}  # nct_id -> eligibility ë§¤í•‘
    
    for eligibility in eligibility_list:
        nct_id = eligibility.get('nct_id')
        criteria_raw = eligibility.get('eligibility_criteria_raw', '')
        inclusion_criteria = format_criteria(eligibility.get('inclusion_criteria'))
        exclusion_criteria = format_criteria(eligibility.get('exclusion_criteria'))
        
        parts = [f"{nct_id}"]
        if criteria_raw:
            parts.append(f"RAW:{criteria_raw}")
        if inclusion_criteria:
            parts.append(f"INC:{inclusion_criteria}")
        if exclusion_criteria:
            parts.append(f"EXC:{exclusion_criteria}")
        item_str = "|".join(parts)
        items.append(item_str)
        nct_id_map[nct_id] = eligibility
    
    # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
    items_text = '\n'.join(items)
    prompt = get_inclusion_exclusion_validation_prompt(items_text)
    
    result = call_gemini_api(prompt)
    
    # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
    if llm_config._all_keys_exhausted:
        # ëª¨ë“  í‚¤ê°€ ì†Œì§„ëœ ê²½ìš° UNCERTAIN ë°˜í™˜
        return {
            nct_id: {
                'status': 'UNCERTAIN',
                'confidence': None,
                'notes': '[API_KEYS_EXHAUSTED] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.'
            }
            for nct_id in nct_id_map.keys()
        }
    
    if not result:
        # API ì‹¤íŒ¨ ì‹œ ëª¨ë‘ UNCERTAIN ì²˜ë¦¬
        return {
            nct_id: {
                'status': 'UNCERTAIN',
                'confidence': None,
                'notes': '[API_FAILED] LLM API í˜¸ì¶œ ì‹¤íŒ¨.'
            }
            for nct_id in nct_id_map.keys()
        }
    
    # ê²°ê³¼ íŒŒì‹±
    results_map = {}
    if isinstance(result, list):
        for r in result:
            nct_id = r.get('nct_id')
            if nct_id in nct_id_map:
                status = r.get('status', '').upper()
                valid_statuses = ['VERIFIED', 'UNCERTAIN', 'INCLUSION_FAILED', 'EXCLUSION_FAILED', 'BOTH_FAILED']
                if status not in valid_statuses:
                    status = 'UNCERTAIN'
                results_map[nct_id] = {
                    'status': status,
                    'confidence': r.get('confidence'),
                    'notes': r.get('notes', '')
                }
        
        # ì‘ë‹µì— ì—†ëŠ” í•­ëª©ì€ UNCERTAIN ì²˜ë¦¬
        for nct_id in nct_id_map.keys():
            if nct_id not in results_map:
                results_map[nct_id] = {
                    'status': 'UNCERTAIN',
                    'confidence': None,
                    'notes': '[PARSE_ERROR] LLM ì‘ë‹µì— nct_idê°€ ì—†ìŒ.'
                }
    else:
        # ë‹¨ì¼ ì‘ë‹µì¸ ê²½ìš° (í•˜ìœ„ í˜¸í™˜ì„±)
        if eligibility_list:
            eligibility = eligibility_list[0]
            nct_id = eligibility.get('nct_id')
            status = result.get('status', '').upper()
            valid_statuses = ['VERIFIED', 'UNCERTAIN', 'INCLUSION_FAILED', 'EXCLUSION_FAILED', 'BOTH_FAILED']
            if status not in valid_statuses:
                status = 'UNCERTAIN'
            results_map[nct_id] = {
                'status': status,
                'confidence': result.get('confidence'),
                'notes': result.get('notes', '')
            }
    
    return results_map


def calculate_consistency_score(validation_results: List[Dict]) -> float:
    """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°: ë™ì¼í•œ ê²°ê³¼ê°€ ë‚˜ì˜¨ ë¹„ìœ¨"""
    if not validation_results:
        return 0.0
    
    status_counts = {}
    for result in validation_results:
        status = result.get('status', '')
        status_counts[status] = status_counts.get(status, 0) + 1
    
    if not status_counts:
        return 0.0
    
    # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ê²°ê³¼ì˜ ë¹„ìœ¨
    max_count = max(status_counts.values())
    return max_count / len(validation_results)


def majority_voting(validation_results: List[Dict]) -> Dict:
    """Majority Voting: ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ê²€ì¦ ìƒíƒœë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì„ íƒ"""
    if not validation_results:
        return {
            'status': 'UNCERTAIN',
            'confidence': None,
            'notes': 'ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
        }
    
    status_counts = {}
    confidences_by_status = {}
    
    for result in validation_results:
        status = result.get('status', '')
        status_counts[status] = status_counts.get(status, 0) + 1
        
        if status not in confidences_by_status:
            confidences_by_status[status] = []
        confidences_by_status[status].append(result.get('confidence'))
    
    if not status_counts:
        return {
            'status': 'UNCERTAIN',
            'confidence': None,
            'notes': 'ìœ íš¨í•œ ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'
        }
    
    # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ìƒíƒœ ì„ íƒ
    max_count = max(status_counts.values())
    final_statuses = [s for s, count in status_counts.items() if count == max_count]
    
    # ë™ë¥  ë°œìƒ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ UNCERTAIN ì²˜ë¦¬
    if len(final_statuses) > 1:
        final_status = 'UNCERTAIN'
        notes = f'[TIE] ë™ë¥  ë°œìƒ: {", ".join(final_statuses)}. ë³´ìˆ˜ì ìœ¼ë¡œ UNCERTAIN ì²˜ë¦¬.'
    else:
        final_status = final_statuses[0]
        notes = f'[MAJORITY] {max_count}/{len(validation_results)}íšŒ ì¼ì¹˜'
    
    # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
    if final_status in confidences_by_status:
        confidences = [float(c) for c in confidences_by_status[final_status] if c is not None]
        avg_confidence = sum(confidences) / len(confidences) if confidences else None
    else:
        # ìµœì¢… ìƒíƒœì˜ ì‹ ë¢°ë„ê°€ ì—†ìœ¼ë©´ ì „ì²´ í‰ê· 
        all_confidences = [float(r.get('confidence')) for r in validation_results if r.get('confidence') is not None]
        avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else None
    
    return {
        'status': final_status,
        'confidence': avg_confidence,
        'notes': notes
    }


def apply_confidence_consistency_filtering(
    final_result: Dict,
    consistency_score: float,
    high_consistency_threshold: float = 0.67,
    high_confidence_threshold: float = 0.80,
    low_confidence_threshold: float = 0.50
) -> Dict:
    """Confidence + Consistency ê¸°ë°˜ í•„í„°ë§ ì ìš©"""
    confidence = final_result.get('confidence')
    if confidence is None:
        confidence = 0.0
    else:
        confidence = float(confidence)
    
    # ìë™ ìˆ˜ìš©: Consistency â‰¥ 0.67 & Avg Confidence â‰¥ 0.80
    if consistency_score >= high_consistency_threshold and confidence >= high_confidence_threshold:
        return {
            **final_result,
            'action': 'ACCEPT',
            'needs_manual_review': False
        }
    
    # ì¶”ê°€ ê²€ì¦: Consistency â‰¥ 0.67 & Avg Confidence 0.50~0.80
    if consistency_score >= high_consistency_threshold and low_confidence_threshold <= confidence < high_confidence_threshold:
        return {
            **final_result,
            'action': 'REVALIDATE',
            'needs_manual_review': False
        }
    
    # ìˆ˜ë™ ê²€í† : Consistency < 0.67 ë˜ëŠ” Avg Confidence < 0.50
    return {
        **final_result,
        'action': 'MANUAL_REVIEW',
        'needs_manual_review': True
    }


def validate_with_multi_run_for_eligibility(
    eligibility: Dict,
    validation_results_by_run: Dict[int, Dict],
    existing_results: List[Dict]
) -> Dict:
    """
    ë‹¨ì¼ eligibilityì— ëŒ€í•´ ë‹¤ì¤‘ ê²€ì¦ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        eligibility: ê²€ì¦í•  eligibility ë°ì´í„°
        validation_results_by_run: {run_number: {nct_id: result}} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        existing_results: ê¸°ì¡´ ê²€ì¦ ì´ë ¥
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    nct_id = eligibility.get('nct_id')
    
    # í•´ë‹¹ eligibilityì˜ ê²€ì¦ ê²°ê³¼ ìˆ˜ì§‘
    new_validation_results = []
    for run_num in sorted(validation_results_by_run.keys()):
        run_results = validation_results_by_run[run_num]
        if nct_id in run_results:
            new_validation_results.append(run_results[nct_id])
    
    # ê¸°ì¡´ ê²°ê³¼ì™€ ìƒˆ ê²°ê³¼ í•©ì¹˜ê¸°
    all_validation_results = existing_results + new_validation_results
    
    if not all_validation_results:
        return {
            'nct_id': nct_id,
            'final_status': 'UNCERTAIN',
            'consistency_score': 0.0,
            'validation_results': [],
            'all_validation_results': [],
            'average_confidence': None,
            'validation_count': 0,
            'needs_manual_review': True,
            'action': 'MANUAL_REVIEW'
        }
    
    # ì „ì²´ ê²°ê³¼ë¡œ Majority Voting
    final_result = majority_voting(all_validation_results)
    
    # ì „ì²´ ê²°ê³¼ë¡œ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
    consistency_score = calculate_consistency_score(all_validation_results)
    
    # ì „ì²´ ê²°ê³¼ë¡œ í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
    all_confidences = [float(r.get('confidence')) for r in all_validation_results if r.get('confidence') is not None]
    avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else None
    
    # Confidence + Consistency ê¸°ë°˜ í•„í„°ë§
    filtered_result = apply_confidence_consistency_filtering(
        final_result,
        consistency_score
    )
    
    return {
        'nct_id': nct_id,
        'final_status': filtered_result['status'],
        'consistency_score': consistency_score,
        'validation_results': new_validation_results,  # ìƒˆë¡œ ìˆ˜í–‰í•œ ê²€ì¦ë§Œ
        'all_validation_results': all_validation_results,  # ì „ì²´ ê²€ì¦ ê²°ê³¼
        'average_confidence': avg_confidence,
        'validation_count': len(all_validation_results),  # ì „ì²´ ê²€ì¦ íšŸìˆ˜
        'needs_manual_review': filtered_result.get('needs_manual_review', False),
        'action': filtered_result.get('action', 'ACCEPT'),
        'llm_validation_confidence': filtered_result.get('confidence'),
        'llm_validation_notes': filtered_result.get('notes', '')
    }


def validate_batch_eligibility(eligibility_list: List[Dict], num_validations: int = 3, conn=None) -> tuple:
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ eligibilityCriteriaë“¤ì„ ë‹¤ì¤‘ ê²€ì¦ (ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ ë°©ì‹)
    
    Args:
        eligibility_list: ê²€ì¦í•  eligibility ë¦¬ìŠ¤íŠ¸
        num_validations: ê° eligibilityë‹¹ ê²€ì¦ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)
        conn: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (ì¬ê²€ì¦ ì‹œ ê¸°ì¡´ ì´ë ¥ê³¼ í•©ì¹˜ê¸° ìœ„í•´ í•„ìš”)
    
    Returns:
        (results: List[Dict], validation_results_by_run: Dict[int, Dict])
        - results: ê° eligibilityë³„ ìµœì¢… ê²€ì¦ ê²°ê³¼
        - validation_results_by_run: {run_number: {nct_id: result}} í˜•íƒœì˜ ê²€ì¦ ì´ë ¥
    
    Note:
        ë°°ì¹˜ ë‚´ ëª¨ë“  í•­ëª©ì„ í•œ ë²ˆì— í”„ë¡¬í”„íŠ¸ë¡œ ë§Œë“¤ì–´ì„œ NíšŒ ê²€ì¦í•©ë‹ˆë‹¤.
        ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë°°ì¹˜ ë‹¨ìœ„ API í˜¸ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        ì¬ê²€ì¦ ì‹œ ê¸°ì¡´ ê²€ì¦ ì´ë ¥ê³¼ í•©ì³ì„œ Majority Votingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if not eligibility_list:
        return [], {}
    
    # ê¸°ì¡´ ê²€ì¦ ì´ë ¥ ì¡°íšŒ (ì¬ê²€ì¦ ì‹œ)
    existing_results_by_eligibility = {}
    if conn:
        for eligibility in eligibility_list:
            nct_id = eligibility.get('nct_id')
            if nct_id:
                existing_results = get_existing_validation_history(conn, nct_id)
                if existing_results:
                    existing_results_by_eligibility[nct_id] = existing_results
    
    # NíšŒ ê²€ì¦ ìˆ˜í–‰ (ë°°ì¹˜ ë‹¨ìœ„ë¡œ)
    validation_results_by_run = {}  # {run_number: {nct_id: result}}
    
    for run_num in range(1, num_validations + 1):
        # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ í™•ì¸
        import llm_config
        if llm_config._all_keys_exhausted:
            print(f"[WARN] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ê²€ì¦ ì¤‘ë‹¨ (run {run_num}/{num_validations})")
            break
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ 1íšŒ ê²€ì¦
        run_results = validate_batch_single_run(eligibility_list)
        validation_results_by_run[run_num] = run_results
        
        # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
        if llm_config._all_keys_exhausted:
            print(f"[WARN] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ê²€ì¦ ì¤‘ë‹¨ (run {run_num}/{num_validations})")
            break
        
        # Rate limiting (ë§ˆì§€ë§‰ ê²€ì¦ ì œì™¸)
        if run_num < num_validations:
            time.sleep(60 / MAX_REQUESTS_PER_MINUTE)
    
    # ê° eligibilityë³„ë¡œ ê²°ê³¼ ì²˜ë¦¬
    results = []
    for eligibility in eligibility_list:
        nct_id = eligibility.get('nct_id')
        existing_results = existing_results_by_eligibility.get(nct_id, [])
        
        result = validate_with_multi_run_for_eligibility(
            eligibility,
            validation_results_by_run,
            existing_results
        )
        results.append(result)
    
    return results, validation_results_by_run


def get_existing_validation_history(conn, nct_id: str) -> List[Dict]:
    """ê¸°ì¡´ ê²€ì¦ ì´ë ¥ì„ ì¡°íšŒ"""
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT 
                validation_status as status,
                validation_confidence as confidence,
                validation_notes as notes
            FROM inclusion_exclusion_llm_validation_history
            WHERE nct_id = %s
            ORDER BY validation_run
        """, (nct_id,))
        results = cur.fetchall()
        return [dict(r) for r in results]


def save_validation_history_batch(conn, validation_results_by_run: Dict[int, Dict]):
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê²€ì¦ ì´ë ¥ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ ë°©ì‹)
    
    Args:
        conn: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        validation_results_by_run: {run_number: {nct_id: result}} í˜•íƒœ
    """
    if not validation_results_by_run:
        return
    
    # ëª¨ë“  nct_id ìˆ˜ì§‘
    all_nct_ids = set()
    for run_results in validation_results_by_run.values():
        all_nct_ids.update(run_results.keys())
    
    if not all_nct_ids:
        return
    
    # ê° nct_idë³„ë¡œ ê¸°ì¡´ ì´ë ¥ì˜ ìµœëŒ€ validation_run ì¡°íšŒ (ë°°ì¹˜ë¡œ)
    nct_id_max_runs = {}
    with conn.cursor() as cur:
        placeholders = ','.join(['%s'] * len(all_nct_ids))
        cur.execute(f"""
            SELECT 
                nct_id,
                COALESCE(MAX(validation_run), 0) as max_run
            FROM inclusion_exclusion_llm_validation_history
            WHERE nct_id IN ({placeholders})
            GROUP BY nct_id
        """, list(all_nct_ids))
        
        for row in cur.fetchall():
            nct_id_max_runs[row[0]] = row[1]
    
    # nct_idë³„ë¡œ ë‹¤ìŒ run ë²ˆí˜¸ ì„¤ì •
    nct_id_next_runs = {}
    for nct_id in all_nct_ids:
        nct_id_next_runs[nct_id] = nct_id_max_runs.get(nct_id, 0) + 1
    
    # ëª¨ë“  ê²€ì¦ ì´ë ¥ ë°ì´í„° ìˆ˜ì§‘
    history_data = []
    for run_num in sorted(validation_results_by_run.keys()):
        run_results = validation_results_by_run[run_num]
        for nct_id, result in run_results.items():
            history_data.append({
                'nct_id': nct_id,
                'validation_run': nct_id_next_runs[nct_id],
                'validation_status': result.get('status'),
                'validation_confidence': result.get('confidence'),
                'validation_notes': result.get('notes', '')
            })
            # ë‹¤ìŒ run ë²ˆí˜¸ ì¦ê°€
            nct_id_next_runs[nct_id] += 1
    
    if not history_data:
        return
    
    # ë°°ì¹˜ë¡œ ì €ì¥
    history_sql = """
        INSERT INTO inclusion_exclusion_llm_validation_history 
        (nct_id, validation_run, validation_status, validation_confidence, validation_notes)
        VALUES (%(nct_id)s, %(validation_run)s, %(validation_status)s, %(validation_confidence)s, %(validation_notes)s)
    """
    
    with conn.cursor() as cur:
        execute_batch(cur, history_sql, history_data, page_size=100)
        conn.commit()


def update_validation_results(conn, results: List[Dict], validation_results_by_run: Dict[int, Dict] = None):
    """
    LLM ê²€ì¦ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸ (ë‹¤ì¤‘ ê²€ì¦ ê²°ê³¼ í¬í•¨)
    ì „ì²˜ë¦¬ì™€ ë™ì¼í•˜ê²Œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        conn: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        results: ê²€ì¦ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        validation_results_by_run: {run_number: {nct_id: result}} í˜•íƒœ (ë°°ì¹˜ ê²€ì¦ ê²°ê³¼)
    """
    if not results:
        return
    
    # ê²€ì¦ ì´ë ¥ ì €ì¥ (ë°°ì¹˜ ë‹¨ìœ„ë¡œ)
    if validation_results_by_run:
        save_validation_history_batch(conn, validation_results_by_run)
    else:
        # ê¸°ì¡´ ë°©ì‹ (ê°œë³„ ê²€ì¦ ê²°ê³¼) - í•˜ìœ„ í˜¸í™˜ì„±
        for result in results:
            nct_id = result.get('nct_id')
            validation_results = result.get('validation_results', [])
            if nct_id and validation_results:
                # ê°œë³„ ì €ì¥ (í•˜ìœ„ í˜¸í™˜ì„±)
                nct_id_max_runs = {}
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT COALESCE(MAX(validation_run), 0) as max_run
                        FROM inclusion_exclusion_llm_validation_history
                        WHERE nct_id = %s
                    """, (nct_id,))
                    result_row = cur.fetchone()
                    start_run = (result_row[0] if result_row else 0) + 1
                
                history_sql = """
                    INSERT INTO inclusion_exclusion_llm_validation_history 
                    (nct_id, validation_run, validation_status, validation_confidence, validation_notes)
                    VALUES (%(nct_id)s, %(validation_run)s, %(validation_status)s, %(validation_confidence)s, %(validation_notes)s)
                """
                
                history_data = []
                for idx, val_result in enumerate(validation_results, start_run):
                    history_data.append({
                        'nct_id': nct_id,
                        'validation_run': idx,
                        'validation_status': val_result.get('status'),
                        'validation_confidence': val_result.get('confidence'),
                        'validation_notes': val_result.get('notes', '')
                    })
                
                with conn.cursor() as cur:
                    execute_batch(cur, history_sql, history_data, page_size=100)
                    conn.commit()
    
    # ë©”ì¸ í…Œì´ë¸” ì—…ë°ì´íŠ¸
    update_sql = """
        UPDATE inclusion_exclusion_llm_preprocessed
        SET 
            llm_validation_status = %(final_status)s,
            llm_validation_confidence = %(llm_validation_confidence)s,
            llm_validation_notes = %(llm_validation_notes)s,
            validation_consistency_score = %(consistency_score)s,
            validation_count = %(validation_count)s,
            needs_manual_review = %(needs_manual_review)s,
            avg_validation_confidence = %(average_confidence)s,
            updated_at = CURRENT_TIMESTAMP
        WHERE nct_id = %(nct_id)s
    """
    
    update_data = []
    for result in results:
        update_data.append({
            'nct_id': result.get('nct_id'),
            'final_status': result.get('final_status'),
            'llm_validation_confidence': result.get('llm_validation_confidence'),
            'llm_validation_notes': result.get('llm_validation_notes', ''),
            'consistency_score': result.get('consistency_score'),
            'validation_count': result.get('validation_count', 1),
            'needs_manual_review': result.get('needs_manual_review', False),
            'average_confidence': result.get('average_confidence')
        })
    
    with conn.cursor() as cur:
        execute_batch(cur, update_sql, update_data, page_size=100)
        conn.commit()


def generate_validation_report(conn, output_dir=None):
    """ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
    if output_dir is None:
        output_dir = os.path.join(ROOT_DIR, 'reports')
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f'{output_dir}/inclusion_exclusion_validation_{timestamp}.md'
    
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        # ì „ì²´ í†µê³„
        cur.execute("""
            SELECT 
                COUNT(*) as total_success,
                COUNT(*) FILTER (WHERE llm_validation_status = 'VERIFIED') as verified,
                COUNT(*) FILTER (WHERE llm_validation_status = 'UNCERTAIN') as uncertain,
                COUNT(*) FILTER (WHERE llm_validation_status = 'INCLUSION_FAILED') as inclusion_failed,
                COUNT(*) FILTER (WHERE llm_validation_status = 'EXCLUSION_FAILED') as exclusion_failed,
                COUNT(*) FILTER (WHERE llm_validation_status = 'BOTH_FAILED') as both_failed,
                AVG(llm_validation_confidence) FILTER (WHERE llm_validation_status = 'VERIFIED') as avg_verified_confidence,
                AVG(llm_validation_confidence) as avg_confidence
            FROM inclusion_exclusion_llm_preprocessed
            WHERE llm_status = 'SUCCESS'
        """)
        stats = cur.fetchone()
        
        # Studyë³„ í†µê³„
        cur.execute("""
            SELECT 
                COUNT(DISTINCT nct_id) AS total_studies,
                COUNT(DISTINCT nct_id) FILTER (WHERE llm_validation_status = 'VERIFIED') AS verified_studies
            FROM inclusion_exclusion_llm_preprocessed
            WHERE llm_status = 'SUCCESS'
        """)
        study_stats = cur.fetchone()
        
        # ìƒíƒœë³„ ìƒì„¸ í†µê³„
        cur.execute("""
            SELECT 
                llm_validation_status,
                COUNT(*) as count,
                AVG(llm_validation_confidence) as avg_confidence,
                MIN(llm_validation_confidence) as min_confidence,
                MAX(llm_validation_confidence) as max_confidence,
                AVG(validation_consistency_score) as avg_consistency,
                MIN(validation_consistency_score) as min_consistency,
                MAX(validation_consistency_score) as max_consistency
            FROM inclusion_exclusion_llm_preprocessed
            WHERE llm_status = 'SUCCESS'
              AND llm_validation_status IS NOT NULL
            GROUP BY llm_validation_status
            ORDER BY count DESC
        """)
        status_stats = cur.fetchall()
        
        # ì¼ê´€ì„± ì ìˆ˜ í†µê³„
        cur.execute("""
            SELECT 
                AVG(validation_consistency_score) as avg_consistency,
                MIN(validation_consistency_score) as min_consistency,
                MAX(validation_consistency_score) as max_consistency,
                COUNT(*) FILTER (WHERE validation_consistency_score >= 0.67) as high_consistency_count,
                COUNT(*) FILTER (WHERE validation_consistency_score < 0.67 AND validation_consistency_score >= 0.33) as medium_consistency_count,
                COUNT(*) FILTER (WHERE validation_consistency_score < 0.33) as low_consistency_count,
                COUNT(*) FILTER (WHERE needs_manual_review = TRUE) as manual_review_count
            FROM inclusion_exclusion_llm_preprocessed
            WHERE llm_status = 'SUCCESS'
              AND validation_consistency_score IS NOT NULL
        """)
        consistency_stats = cur.fetchone()
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('# Inclusion/Exclusion LLM ì „ì²˜ë¦¬ ì„±ê³µ í•­ëª© ê²€ì¦ ë¦¬í¬íŠ¸\n\n')
        f.write(f'ìƒì„±ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\n')
        
        f.write('## 1. ì „ì²´ í†µê³„\n\n')
        if stats and stats['total_success'] > 0:
            total = stats['total_success']
            f.write(f'- **ì „ì²´ SUCCESS í•­ëª©**: {total:,}ê°œ\n')
            f.write(f'- **VERIFIED**: {stats["verified"]:,}ê°œ ({stats["verified"]/total*100:.2f}%)\n')
            f.write(f'- **UNCERTAIN**: {stats["uncertain"]:,}ê°œ ({stats["uncertain"]/total*100:.2f}%)\n')
            f.write(f'- **INCLUSION_FAILED**: {stats["inclusion_failed"]:,}ê°œ ({stats["inclusion_failed"]/total*100:.2f}%)\n')
            f.write(f'- **EXCLUSION_FAILED**: {stats["exclusion_failed"]:,}ê°œ ({stats["exclusion_failed"]/total*100:.2f}%)\n')
            f.write(f'- **BOTH_FAILED**: {stats["both_failed"]:,}ê°œ ({stats["both_failed"]/total*100:.2f}%)\n')
            if stats['avg_verified_confidence']:
                f.write(f'- **VERIFIED í‰ê·  ì‹ ë¢°ë„**: {float(stats["avg_verified_confidence"]):.2f}\n')
            if stats['avg_confidence']:
                f.write(f'- **ì „ì²´ í‰ê·  ì‹ ë¢°ë„**: {float(stats["avg_confidence"]):.2f}\n')
        f.write('\n')
        
        f.write('## 2. Studyë³„ í†µê³„\n\n')
        if study_stats:
            total_studies = study_stats['total_studies'] or 0
            verified_studies = study_stats['verified_studies'] or 0
            if total_studies > 0:
                f.write(f'- **ì „ì²´ Study**: {total_studies:,}ê°œ\n')
                f.write(f'- **VERIFIED Study**: {verified_studies:,}ê°œ ({verified_studies/total_studies*100:.2f}%)\n')
        f.write('\n')
        
        f.write('## 3. ìƒíƒœë³„ ìƒì„¸ í†µê³„\n\n')
        if status_stats:
            f.write('| ê²€ì¦ ìƒíƒœ | ê°œìˆ˜ | ë¹„ìœ¨ | í‰ê·  ì‹ ë¢°ë„ | ìµœì†Œ ì‹ ë¢°ë„ | ìµœëŒ€ ì‹ ë¢°ë„ | í‰ê·  ì¼ê´€ì„± | ìµœì†Œ ì¼ê´€ì„± | ìµœëŒ€ ì¼ê´€ì„± |\n')
            f.write('|----------|------|------|------------|------------|------------|------------|------------|------------|\n')
            total_validated = sum(s['count'] for s in status_stats)
            for stat in status_stats:
                percentage = stat['count'] / total_validated * 100 if total_validated > 0 else 0
                avg_conf = float(stat['avg_confidence']) if stat['avg_confidence'] else 0
                min_conf = float(stat['min_confidence']) if stat['min_confidence'] else 0
                max_conf = float(stat['max_confidence']) if stat['max_confidence'] else 0
                avg_cons = float(stat['avg_consistency']) if stat['avg_consistency'] else 0
                min_cons = float(stat['min_consistency']) if stat['min_consistency'] else 0
                max_cons = float(stat['max_consistency']) if stat['max_consistency'] else 0
                f.write(f"| {stat['llm_validation_status']} | {stat['count']:,} | {percentage:.2f}% | {avg_conf:.2f} | {min_conf:.2f} | {max_conf:.2f} | {avg_cons:.2f} | {min_cons:.2f} | {max_cons:.2f} |\n")
        f.write('\n')
        
        f.write('## 4. ì¼ê´€ì„± ì ìˆ˜ í†µê³„\n\n')
        if consistency_stats:
            total_with_consistency = (
                (consistency_stats['high_consistency_count'] or 0) +
                (consistency_stats['medium_consistency_count'] or 0) +
                (consistency_stats['low_consistency_count'] or 0)
            )
            if total_with_consistency > 0:
                f.write(f'- **í‰ê·  ì¼ê´€ì„± ì ìˆ˜**: {float(consistency_stats["avg_consistency"]):.2f}\n')
                f.write(f'- **ìµœì†Œ ì¼ê´€ì„± ì ìˆ˜**: {float(consistency_stats["min_consistency"]):.2f}\n')
                f.write(f'- **ìµœëŒ€ ì¼ê´€ì„± ì ìˆ˜**: {float(consistency_stats["max_consistency"]):.2f}\n')
                f.write(f'\n')
                f.write(f'- **ë†’ì€ ì¼ê´€ì„± (â‰¥0.67)**: {consistency_stats["high_consistency_count"]:,}ê°œ ({consistency_stats["high_consistency_count"]/total_with_consistency*100:.2f}%)\n')
                f.write(f'- **ì¤‘ê°„ ì¼ê´€ì„± (0.33~0.67)**: {consistency_stats["medium_consistency_count"]:,}ê°œ ({consistency_stats["medium_consistency_count"]/total_with_consistency*100:.2f}%)\n')
                f.write(f'- **ë‚®ì€ ì¼ê´€ì„± (<0.33)**: {consistency_stats["low_consistency_count"]:,}ê°œ ({consistency_stats["low_consistency_count"]/total_with_consistency*100:.2f}%)\n')
                f.write(f'\n')
                f.write(f'- **ìˆ˜ë™ ê²€í†  í•„ìš”**: {consistency_stats["manual_review_count"]:,}ê°œ\n')
        f.write('\n')
        
        f.write('## 5. ê²€ì¦ ë°©ë²•\n\n')
        f.write('1. `inclusion_exclusion_llm_preprocessed` í…Œì´ë¸”ì—ì„œ `llm_status = \'SUCCESS\'`ì¸ í•­ëª©ë“¤ì„ ì¡°íšŒ\n')
        f.write('2. ê° í•­ëª©ì„ **ë‹¤ì¤‘ ê²€ì¦** (ê¸°ë³¸ 3íšŒ) ìˆ˜í–‰\n')
        f.write('3. **Majority Voting**ìœ¼ë¡œ ìµœì¢… ê²€ì¦ ìƒíƒœ ê²°ì •\n')
        f.write('4. **ì¼ê´€ì„± ì ìˆ˜** ê³„ì‚° (ë™ì¼ ê²°ê³¼ê°€ ë‚˜ì˜¨ ë¹„ìœ¨)\n')
        f.write('5. **Confidence + Consistency ê¸°ë°˜ í•„í„°ë§** ì ìš©:\n')
        f.write('   - Consistency â‰¥ 0.67 & Avg Confidence â‰¥ 0.80: ìë™ ìˆ˜ìš©\n')
        f.write('   - Consistency â‰¥ 0.67 & Avg Confidence 0.50~0.80: ì¶”ê°€ ê²€ì¦\n')
        f.write('   - Consistency < 0.67 ë˜ëŠ” Avg Confidence < 0.50: ìˆ˜ë™ ê²€í† \n')
        f.write('6. ê²€ì¦ ê²°ê³¼ ë° ì´ë ¥ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n')
        f.write('\n')
    
    print(f"[OK] ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    return report_path


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    print("=" * 80)
    print("[START] LLM ì „ì²˜ë¦¬ ì„±ê³µ í•­ëª© ê²€ì¦ ì‹œì‘ (Inclusion/Exclusion)")
    print("=" * 80)
    
    api_keys = get_api_keys()
    if not api_keys:
        print("\n[ERROR] GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("í™˜ê²½ë³€ìˆ˜ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì— ì¶”ê°€í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    # ì‚¬ìš©ë²•: python llm_validate_inclusion_exclusion.py [limit] [num_validations] [batch_size] [start_batch]
    limit = None
    num_validations = 3  # ê¸°ë³¸ê°’: 3íšŒ
    custom_batch_size = None
    start_batch = 1
    
    num_args = []
    for arg in sys.argv[1:]:
        try:
            num_args.append(int(arg))
        except ValueError:
            pass
    
    if len(num_args) > 0:
        limit = num_args[0]
    if len(num_args) > 1:
        num_validations = num_args[1]
    if len(num_args) > 2:
        custom_batch_size = num_args[2]
    if len(num_args) > 3:
        start_batch = num_args[3]
        if start_batch < 1:
            start_batch = 1
    
    print(f"\n[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {len(api_keys)}ê°œ")
    print(f"[INFO] ì‚¬ìš© ëª¨ë¸: {GEMINI_MODEL}")
    print(f"[INFO] ë‹¤ì¤‘ ê²€ì¦ íšŸìˆ˜: {num_validations}íšŒ")
    
    # ë°°ì¹˜ í¬ê¸° ì¡°ì •
    if custom_batch_size and custom_batch_size > 0:
        import llm_config
        llm_config.BATCH_SIZE = custom_batch_size
        print(f"[INFO] ë°°ì¹˜ í¬ê¸°ë¥¼ {custom_batch_size}ê°œë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.")
    
    if start_batch > 1:
        print(f"[INFO] ë°°ì¹˜ {start_batch}ë²ˆë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
    
    try:
        conn = get_db_connection()
        
        # SUCCESS í•­ëª© ì¡°íšŒ (ì¬ê²€ì¦ í¬í•¨)
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            query = """
                SELECT 
                    nct_id,
                    eligibility_criteria_raw,
                    inclusion_criteria,
                    exclusion_criteria
                FROM inclusion_exclusion_llm_preprocessed
                WHERE llm_status = 'SUCCESS'
                ORDER BY nct_id
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cur.execute(query)
            eligibility_list = cur.fetchall()
        
        total_count = len(eligibility_list)
        print(f"\n[INFO] ì²˜ë¦¬í•  SUCCESS í•­ëª©: {total_count:,}ê°œ")
        
        if total_count == 0:
            print("[INFO] ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            # ë¦¬í¬íŠ¸ë§Œ ìƒì„±
            print("\n[STEP] ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
            generate_validation_report(conn)
            conn.close()
            return
        
        # LLM ë‹¤ì¤‘ ê²€ì¦ (ë°°ì¹˜ ì²˜ë¦¬)
        import llm_config
        actual_batch_size = llm_config.BATCH_SIZE
        print(f"\n[STEP 1] LLM ë‹¤ì¤‘ ê²€ì¦ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {actual_batch_size}, í•­ëª©ë‹¹ {num_validations}íšŒ ê²€ì¦)...")
        
        verified_count = 0
        uncertain_count = 0
        inclusion_failed_count = 0
        exclusion_failed_count = 0
        both_failed_count = 0
        manual_review_count = 0
        high_consistency_count = 0
        medium_consistency_count = 0
        low_consistency_count = 0
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for batch_start in range(0, total_count, actual_batch_size):
            batch_end = min(batch_start + actual_batch_size, total_count)
            batch_eligibility = eligibility_list[batch_start:batch_end]
            batch_num = (batch_start // actual_batch_size) + 1
            total_batches = (total_count + actual_batch_size - 1) // actual_batch_size
            
            # start_batch ì˜µì…˜: ì§€ì •ëœ ë°°ì¹˜ë¶€í„° ì‹œì‘
            if batch_num < start_batch:
                print(f"  ë°°ì¹˜ {batch_num}/{total_batches} ê±´ë„ˆëœ€ (start_batch={start_batch})")
                continue
            
            print(f"  ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘: {batch_start + 1:,}~{batch_end:,}ë²ˆì§¸ í•­ëª©")
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ í™•ì¸
            import llm_config
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‹¤ì¤‘ ê²€ì¦ ìˆ˜í–‰ (ê¸°ì¡´ ì´ë ¥ê³¼ í•©ì¹˜ê¸° ìœ„í•´ conn ì „ë‹¬)
            batch_results, validation_results_by_run = validate_batch_eligibility(batch_eligibility, num_validations, conn)
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            # ê²°ê³¼ ì§‘ê³„
            for result in batch_results:
                status = result.get('final_status', '')
                consistency = result.get('consistency_score', 0.0)
                
                if status == 'VERIFIED':
                    verified_count += 1
                elif status == 'UNCERTAIN':
                    uncertain_count += 1
                elif status == 'INCLUSION_FAILED':
                    inclusion_failed_count += 1
                elif status == 'EXCLUSION_FAILED':
                    exclusion_failed_count += 1
                elif status == 'BOTH_FAILED':
                    both_failed_count += 1
                
                if result.get('needs_manual_review', False):
                    manual_review_count += 1
                
                if consistency >= 0.67:
                    high_consistency_count += 1
                elif consistency >= 0.33:
                    medium_consistency_count += 1
                else:
                    low_consistency_count += 1
            
            # Rate limiting (ë°°ì¹˜ ê°„ ëŒ€ê¸°)
            time.sleep(60 / MAX_REQUESTS_PER_MINUTE)
            
            # ë°°ì¹˜ë§ˆë‹¤ DB ì €ì¥
            if batch_results:
                print(f"  ë°°ì¹˜ {batch_num} ê²°ê³¼ ì €ì¥ ì¤‘... ({len(batch_results)}ê°œ)")
                update_validation_results(conn, batch_results, validation_results_by_run)
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆìœ¼ë©´ ë°°ì¹˜ ë£¨í”„ë„ ì¤‘ë‹¨
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
        
        print(f"\n[INFO] ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  ì „ì²´: {total_count:,}ê°œ")
        print(f"  VERIFIED: {verified_count:,}ê°œ ({verified_count/total_count*100:.1f}%)")
        print(f"  UNCERTAIN: {uncertain_count:,}ê°œ ({uncertain_count/total_count*100:.1f}%)")
        print(f"  INCLUSION_FAILED: {inclusion_failed_count:,}ê°œ ({inclusion_failed_count/total_count*100:.1f}%)")
        print(f"  EXCLUSION_FAILED: {exclusion_failed_count:,}ê°œ ({exclusion_failed_count/total_count*100:.1f}%)")
        print(f"  BOTH_FAILED: {both_failed_count:,}ê°œ ({both_failed_count/total_count*100:.1f}%)")
        print(f"\n[INFO] ì¼ê´€ì„± ì ìˆ˜ ë¶„í¬:")
        print(f"  ë†’ì€ ì¼ê´€ì„± (â‰¥0.67): {high_consistency_count:,}ê°œ ({high_consistency_count/total_count*100:.1f}%)")
        print(f"  ì¤‘ê°„ ì¼ê´€ì„± (0.33~0.67): {medium_consistency_count:,}ê°œ ({medium_consistency_count/total_count*100:.1f}%)")
        print(f"  ë‚®ì€ ì¼ê´€ì„± (<0.33): {low_consistency_count:,}ê°œ ({low_consistency_count/total_count*100:.1f}%)")
        print(f"\n[INFO] ìˆ˜ë™ ê²€í†  í•„ìš”: {manual_review_count:,}ê°œ ({manual_review_count/total_count*100:.1f}%)")
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        print("\n[STEP 2] ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        generate_validation_report(conn)
        
        conn.close()
        
    except Exception as e:
        print(f"\n[ERROR] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        if 'conn' in locals():
            conn.close()


if __name__ == "__main__":
    main()

