"""
ì„±ê³µ í•­ëª© LLM ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

outcome_normalized_success í…Œì´ë¸”ì—ì„œ ì„±ê³µ í•­ëª©ì„ ì¶”ì¶œí•˜ì—¬
Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì‹± ê²°ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import json
import time
from typing import Dict, Optional, List
import psycopg2
from psycopg2.extras import RealDictCursor, execute_batch
from dotenv import load_dotenv
from llm_config import (
    get_api_keys, get_client, switch_to_next_api_key, GEMINI_MODEL,
    MAX_REQUESTS_PER_MINUTE, BATCH_SIZE, MAX_RETRIES, RETRY_DELAY
)
from llm_prompts import get_validation_prompt

load_dotenv()

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432'),
    'database': os.getenv('DB_NAME', 'clinicaltrials'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', '')
}


def get_db_connection():
    """PostgreSQL ì—°ê²° ìƒì„±"""
    return psycopg2.connect(**DB_CONFIG)


def call_gemini_api(prompt: str) -> Optional[Dict]:
    """Gemini API í˜¸ì¶œ (ì—¬ëŸ¬ API í‚¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„, 429 ì—ëŸ¬ ì‹œ ìë™ ì „í™˜)"""
    api_keys = get_api_keys()
    if not api_keys:
        print("[ERROR] GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        return None
    
    # í˜„ì¬ ì „ì—­ í‚¤ ì¸ë±ìŠ¤ë¶€í„° ì‹œì‘
    import llm_config
    start_key_index = llm_config._current_key_index
    
    last_error = None
    
    # ëª¨ë“  í‚¤ë¥¼ ì‹œë„ (6ê°œ í‚¤ë©´ 0,1,2,3,4,5 ì´ 6ë²ˆ)
    for attempt in range(len(api_keys)):
        key_index = (start_key_index + attempt) % len(api_keys)
        
        try:
            # íŠ¹ì • í‚¤ë¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            from google import genai
            client = genai.Client(api_key=api_keys[key_index])
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt
            )
            
            # ì„±ê³µ ì‹œ ì „ì—­ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (í‚¤ ë³€ê²½ ì‹œ ê·œì¹™ ë‹¤ì‹œ ë³´ë‚´ê¸° ìœ„í•´)
            if llm_config._current_key_index != key_index:
                llm_config._previous_key_index = llm_config._current_key_index
            llm_config._current_key_index = key_index
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content = response.text.strip()
            
            # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
            if content.startswith('```'):
                # ì½”ë“œ ë¸”ë¡ ì œê±°
                lines = content.split('\n')
                content = '\n'.join(lines[1:-1]) if len(lines) > 2 else content
            
            try:
                parsed = json.loads(content)
                return parsed
            except json.JSONDecodeError as e:
                print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨ (í‚¤ {key_index + 1}/{len(api_keys)}): {e}")
                print(f"  ì‘ë‹µ ë‚´ìš©: {content[:200]}")
                return None
            
        except Exception as e:
            error_str = str(e)
            last_error = e
            
            # 429 ì—ëŸ¬ (RESOURCE_EXHAUSTED) ì²´í¬
            if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str.upper():
                print(f"âš ï¸  API í‚¤ {key_index + 1}/{len(api_keys)}ì—ì„œ 429 ì—ëŸ¬ ë°œìƒ (ì‹œë„ {attempt + 1}/{len(api_keys)}): {error_str}")
                
                # ë§ˆì§€ë§‰ ì‹œë„(6ê°œ í‚¤ë©´ attempt==5)ì¸ ê²½ìš° ì¢…ë£Œ
                if attempt == len(api_keys) - 1:
                    print(f"[ERROR] ëª¨ë“  API í‚¤({len(api_keys)}ê°œ)ê°€ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    llm_config._current_key_index = key_index
                    llm_config._all_keys_exhausted = True
                    return None
                else:
                    next_key_index = (start_key_index + attempt + 1) % len(api_keys)
                    print(f"ğŸ”„ ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜í•©ë‹ˆë‹¤ (í‚¤ {next_key_index + 1}/{len(api_keys)})")
                    continue  # ë‹¤ìŒ í‚¤ë¡œ ì¬ì‹œë„
            else:
                # 429ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì—ëŸ¬ëŠ” í˜„ì¬ í‚¤ì—ì„œ ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë°˜í™˜
                print(f"[ERROR] Gemini API ì˜¤ë¥˜ (í‚¤ {key_index + 1}/{len(api_keys)}): {error_str}")
                return None
    
    # ëª¨ë“  í‚¤ ì‹œë„ ì‹¤íŒ¨ (ì´ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šì•„ì•¼ í•¨)
    print(f"[ERROR] ëª¨ë“  API í‚¤({len(api_keys)}ê°œ) ì‹œë„ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì—ëŸ¬: {str(last_error)}")
    llm_config._all_keys_exhausted = True
    return None


def format_time_points(time_points) -> str:
    """time_pointsë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if not time_points:
        return ''
    try:
        if isinstance(time_points, str):
            time_points = json.loads(time_points)
        if isinstance(time_points, list):
            return ', '.join([f"{tp.get('value', '')} {tp.get('unit', '')}" for tp in time_points if tp])
    except:
        pass
    return str(time_points)


def validate_batch_outcomes(outcomes: List[Dict]) -> List[Dict]:
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ outcomeë“¤ì„ LLMìœ¼ë¡œ ê²€ì¦ (ê·œì¹™ ìºì‹± ì ìš©)"""
    if not outcomes:
        return []
    
    import llm_config
    
    # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìµœì†Œí™”)
    items = []
    for outcome in outcomes:
        oid = outcome['outcome_id']
        mr = outcome.get('measure_raw', '')
        tr = outcome.get('time_frame_raw', '')
        mc = outcome.get('measure_code', '')
        tv = outcome.get('time_value_main', '')
        tu = outcome.get('time_unit_main', '')
        tp = format_time_points(outcome.get('time_points'))
        # ë¹ˆ ê°’ ìƒëµí•˜ì—¬ ë” ì§§ê²Œ
        parts = [f"{oid}"]
        if mr: parts.append(f"M:{mr}")
        if tr: parts.append(f"T:{tr}")
        if mc: parts.append(f"C:{mc}")
        if tv and tu: parts.append(f"V:{tv}{tu}")
        if tp: parts.append(f"P:{tp}")
        item_str = "|".join(parts)
        items.append(item_str)
    
    # API í‚¤ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ê·œì¹™ ìºì‹±)
    current_key_idx = llm_config._current_key_index
    previous_key_idx = llm_config._previous_key_index
    include_rules = (current_key_idx != previous_key_idx or previous_key_idx == -1)
    
    # í‚¤ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ í˜¸ì¶œì„ ìœ„í•´)
    if include_rules:
        llm_config._previous_key_index = current_key_idx
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    items_text = '\n'.join(items)
    prompt = get_validation_prompt(items_text)
    
    result = call_gemini_api(prompt)
    
    if not result:
        # API ì‹¤íŒ¨ ì‹œ ëª¨ë‘ UNCERTAIN ì²˜ë¦¬
        return [{
            'outcome_id': outcome['outcome_id'],
            'llm_validation_status': 'UNCERTAIN',
            'llm_validation_confidence': None,
            'llm_validation_notes': 'API í˜¸ì¶œ ì‹¤íŒ¨'
        } for outcome in outcomes]
    
    # ê²°ê³¼ íŒŒì‹± (ë°°ì—´ë¡œ ì‘ë‹µ ë°›ìŒ)
    results = []
    if isinstance(result, list):
        # outcome_idë¡œ ë§¤í•‘
        result_map = {r.get('outcome_id'): r for r in result if 'outcome_id' in r}
        for outcome in outcomes:
            outcome_id = outcome['outcome_id']
            if outcome_id in result_map:
                r = result_map[outcome_id]
                status = r.get('status', '').upper()
                valid_statuses = ['VERIFIED', 'UNCERTAIN', 'MEASURE_FAILED', 'TIMEFRAME_FAILED', 'BOTH_FAILED']
                if status not in valid_statuses:
                    status = 'UNCERTAIN'
                results.append({
                    'outcome_id': outcome_id,
                    'llm_validation_status': status,
                    'llm_validation_confidence': r.get('confidence'),
                    'llm_validation_notes': r.get('notes')
                })
            else:
                results.append({
                    'outcome_id': outcome_id,
                    'llm_validation_status': 'UNCERTAIN',
                    'llm_validation_confidence': None,
                    'llm_validation_notes': 'ì‘ë‹µì— outcome_id ì—†ìŒ'
                })
    else:
        # ë‹¨ì¼ ì‘ë‹µì¸ ê²½ìš° (í•˜ìœ„ í˜¸í™˜ì„±)
        status = result.get('status', '').upper()
        valid_statuses = ['VERIFIED', 'UNCERTAIN', 'MEASURE_FAILED', 'TIMEFRAME_FAILED', 'BOTH_FAILED']
        if status not in valid_statuses:
            status = 'UNCERTAIN'
        if outcomes:
            results.append({
                'outcome_id': outcomes[0]['outcome_id'],
                'llm_validation_status': status,
                'llm_validation_confidence': result.get('confidence'),
                'llm_validation_notes': result.get('notes')
            })
    
    return results


def update_validation_results(conn, results: List[Dict]):
    """LLM ê²€ì¦ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸"""
    if not results:
        return
    
    update_sql = """
        UPDATE outcome_normalized
        SET 
            llm_validation_status = %(llm_validation_status)s,
            llm_validation_confidence = %(llm_validation_confidence)s,
            llm_validation_notes = %(llm_validation_notes)s
        WHERE outcome_id = %(outcome_id)s
    """
    
    with conn.cursor() as cur:
        execute_batch(cur, update_sql, results, page_size=100)
        conn.commit()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    print("=" * 80)
    print("[START] ì„±ê³µ í•­ëª© LLM ê²€ì¦ ì‹œì‘")
    print("=" * 80)
    
    api_keys = get_api_keys()
    if not api_keys:
        print("\n[ERROR] GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("í™˜ê²½ë³€ìˆ˜ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì— ì¶”ê°€í•˜ì„¸ìš”.")
        print("ì—¬ëŸ¬ í‚¤ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ GEMINI_API_KEY_2, GEMINI_API_KEY_3 ë“±ì„ ì¶”ê°€í•˜ì„¸ìš”")
        sys.exit(1)
    
    print(f"\n[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {len(api_keys)}ê°œ")
    print(f"[INFO] ì‚¬ìš© ëª¨ë¸: {GEMINI_MODEL}")
    
    # ì²˜ë¦¬í•  í•­ëª© ìˆ˜ ì œí•œ (ì˜µì…˜)
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else None
    
    try:
        conn = get_db_connection()
        
        # ì„±ê³µ í•­ëª© ì¡°íšŒ (ì•„ì§ ê²€ì¦í•˜ì§€ ì•Šì€ í•­ëª©) - ì „ì²´ ì¡°íšŒ
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            query = """
                SELECT 
                    o.outcome_id,
                    o.measure_raw,
                    o.time_frame_raw,
                    o.measure_code,
                    o.time_value_main,
                    o.time_unit_main,
                    o.time_points
                FROM outcome_normalized_success o
                WHERE o.llm_validation_status IS NULL
                ORDER BY o.outcome_id
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cur.execute(query)
            success_outcomes = cur.fetchall()
        
        total_count = len(success_outcomes)
        print(f"\n[INFO] ì²˜ë¦¬í•  ì„±ê³µ í•­ëª©: {total_count:,}ê°œ")
        
        if total_count == 0:
            print("[INFO] ì²˜ë¦¬í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            conn.close()
            return
        
        # LLM ê²€ì¦ (ë°°ì¹˜ ì²˜ë¦¬)
        print("\n[STEP 1] LLM ê²€ì¦ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {})...".format(BATCH_SIZE))
        verified_count = 0
        uncertain_count = 0
        measure_failed_count = 0
        timeframe_failed_count = 0
        both_failed_count = 0
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ì—ì„œ ë‚˜ëˆ„ê¸°)
        for batch_start in range(0, total_count, BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, total_count)
            batch_outcomes = success_outcomes[batch_start:batch_end]
            batch_num = (batch_start // BATCH_SIZE) + 1
            total_batches = (total_count + BATCH_SIZE - 1) // BATCH_SIZE
            
            print(f"  ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘: {batch_start + 1:,}~{batch_end:,}ë²ˆì§¸ í•­ëª©")
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ í™•ì¸
            import llm_config
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í•œë²ˆì— API í˜¸ì¶œ
            batch_results = validate_batch_outcomes(batch_outcomes)
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸ (í˜¸ì¶œ ì¤‘ì— ì†Œì§„ë  ìˆ˜ ìˆìŒ)
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            # ê²°ê³¼ ì§‘ê³„
            for result in batch_results:
                status = result['llm_validation_status']
                if status == 'VERIFIED':
                    verified_count += 1
                elif status == 'UNCERTAIN':
                    uncertain_count += 1
                elif status == 'MEASURE_FAILED':
                    measure_failed_count += 1
                elif status == 'TIMEFRAME_FAILED':
                    timeframe_failed_count += 1
                elif status == 'BOTH_FAILED':
                    both_failed_count += 1
            
            # Rate limiting (ë°°ì¹˜ë‹¹ 1íšŒ í˜¸ì¶œì´ë¯€ë¡œ ë‹¨ìˆœí™”)
            time.sleep(60 / MAX_REQUESTS_PER_MINUTE)  # ë¶„ë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ
            
            # ë°°ì¹˜ë§ˆë‹¤ DB ì—…ë°ì´íŠ¸
            if batch_results:
                print(f"  ë°°ì¹˜ {batch_num} ê²°ê³¼ ì €ì¥ ì¤‘... ({len(batch_results)}ê°œ)")
                update_validation_results(conn, batch_results)
            
            print(f"  ë°°ì¹˜ {batch_num} ì™„ë£Œ (VERIFIED: {verified_count:,}, UNCERTAIN: {uncertain_count:,}, MEASURE_FAILED: {measure_failed_count:,}, TIMEFRAME_FAILED: {timeframe_failed_count:,}, BOTH_FAILED: {both_failed_count:,})")
            
            # ëª¨ë“  í‚¤ê°€ ì†Œì§„ë˜ì—ˆìœ¼ë©´ ë°°ì¹˜ ë£¨í”„ë„ ì¤‘ë‹¨
            import llm_config
            if llm_config._all_keys_exhausted:
                print(f"\n[ERROR] ëª¨ë“  API í‚¤ê°€ ì†Œì§„ë˜ì–´ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
        
        print(f"\n[INFO] ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  ì „ì²´: {total_count:,}ê°œ")
        print(f"  VERIFIED (ì™„ë²½): {verified_count:,}ê°œ ({verified_count/total_count*100:.1f}%)")
        print(f"  UNCERTAIN (ì• ë§¤): {uncertain_count:,}ê°œ ({uncertain_count/total_count*100:.1f}%)")
        print(f"  MEASURE_FAILED (Measure ë¶ˆì¼ì¹˜): {measure_failed_count:,}ê°œ ({measure_failed_count/total_count*100:.1f}%)")
        print(f"  TIMEFRAME_FAILED (Timeframe ë¶ˆì¼ì¹˜): {timeframe_failed_count:,}ê°œ ({timeframe_failed_count/total_count*100:.1f}%)")
        print(f"  BOTH_FAILED (ë‘˜ ë‹¤ ë¶ˆì¼ì¹˜): {both_failed_count:,}ê°œ ({both_failed_count/total_count*100:.1f}%)")
        
        conn.close()
        
    except Exception as e:
        print(f"\n[ERROR] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        if 'conn' in locals():
            conn.close()


if __name__ == "__main__":
    main()

